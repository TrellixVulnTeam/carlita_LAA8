{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import threading\n",
    "\n",
    "from train_utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from IPython.core.display import Image as JupyterImage\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "deg_to_rad = lambda x: x*0.0174533\n",
    "\n",
    "crop = transforms.CenterCrop(56)\n",
    "resize = transforms.Resize(64)\n",
    "color_jitter = transforms.ColorJitter(brightness=.5, contrast=.5, saturation=.5, hue=.5)\n",
    "\n",
    "def get_rotated_view(front, aux, rotation):\n",
    "    front = front.clone()\n",
    "    SEQ_LEN, BS, C, H, W = front.shape\n",
    "    ff = front.reshape(SEQ_LEN*BS, C, H, W)\n",
    "    ff = transforms.RandomRotation([rotation,rotation], expand=False, center=None, fill=None)(ff)\n",
    "    ff = crop(ff)\n",
    "    ff = resize(ff)\n",
    "    #ff = color_jitter(ff)\n",
    "    ff = ff.reshape(SEQ_LEN, BS, C, H, W)\n",
    "    _aux = aux.clone()\n",
    "    _aux[:,:,0] -= deg_to_rad(rotation)\n",
    "    return ff, _aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym3\n",
    "from procgen import ProcgenGym3Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "\n",
    "# num_levels=1, start_level=6\n",
    "env = ProcgenGym3Env(num=bs, env_name=\"testgame\", render_mode='rgb_array')\n",
    "env = gym3.ViewerWrapper(env, info_key=\"rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:222] . file not found: archive/data/93987830202736",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9ecc62f6c6a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVizCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_rnn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"m_cons_3.28b.torch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/procgen/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/procgen/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/procgen/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/procgen/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:222] . file not found: archive/data/93987830202736"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "m = VizCNN(use_rnn=False).to(device);\n",
    "m.load_state_dict(torch.load(\"m_cons_3.28b.torch\"))\n",
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = get_hidden(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING DAGGER\n",
      "SETTING DAGGER\n",
      "SETTING DAGGER\n",
      "SETTING DAGGER\n",
      "SETTING DAGGER\n",
      "SETTING DAGGER\n",
      "SETTING DAGGER\n",
      "SETTING DAGGER\n",
      "SETTING DAGGER\n",
      "SETTING DAGGER\n",
      "CPU times: user 2.75 s, sys: 174 ms, total: 2.92 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "use_autopilot = True\n",
    "\n",
    "s = np.array([[.1, .2] for _ in range(bs)], dtype=np.float32)\n",
    "daggerized_controls = s\n",
    "\n",
    "\n",
    "seq_len = 1000\n",
    "DAGGER_CADENCE = 100\n",
    "DAGGER_DURATION = 10\n",
    "do_dagger = False\n",
    "dagger_counter = 0\n",
    "TRAINING_WHEELS_WINDOW = 20\n",
    "use_training_wheels = False\n",
    "\n",
    "act_grads = []\n",
    "imgs = []\n",
    "salmaps = []\n",
    "\n",
    "#with torch.no_grad():\n",
    "for i in range(seq_len):\n",
    "    env.act(s)\n",
    "    rew, obs, first = env.observe()\n",
    "    img = obs['rgb']\n",
    "    info = env.get_info()\n",
    "\n",
    "    aux = np.array([[e[a] for a in aux_properties] for e in info])\n",
    "    autopilot_controls = np.array([[e['autopilot_steer'], e['autopilot_throttle']] for e in info])\n",
    "\n",
    "    front = torch.from_numpy(img.astype(np.float32)/255.).unsqueeze(0).permute(0,1,4,2,3)\n",
    "    aux = torch.from_numpy(aux.astype(np.float32)).unsqueeze(0)\n",
    "\n",
    "    if use_autopilot:\n",
    "        if i % DAGGER_CADENCE == 0:\n",
    "            print('SETTING DAGGER')\n",
    "            dagger_counter = 0\n",
    "            steer_aug = random.uniform(-.3, .3)\n",
    "            #throttle_aug = random.uniform(.5, 1.5)\n",
    "            daggerized_controls = np.array([[c[0]+steer_aug, c[1]] for c in autopilot_controls])\n",
    "            do_dagger = True\n",
    "\n",
    "        if do_dagger:\n",
    "            s = daggerized_controls\n",
    "            dagger_counter+=1\n",
    "        else:\n",
    "            s = autopilot_controls\n",
    "\n",
    "        if dagger_counter == DAGGER_DURATION:\n",
    "            do_dagger = False\n",
    "\n",
    "    else:\n",
    "        front = front.to(device)\n",
    "        aux = aux.to(device)\n",
    "        out,hidden, salmap = m(front, aux, hidden, return_salmap=True, register_activations=True)\n",
    "        out[:,:,1]=.6\n",
    "        s = out.squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        \n",
    "        \"\"\"        baseline = torch.zeros_like(front)\n",
    "        steps = 100\n",
    "        grads = []\n",
    "\n",
    "        for i in range(0,steps+1):\n",
    "            mixed_img = baseline + (float(i)/steps)*(front-baseline)\n",
    "            m.zero_grad()\n",
    "            out, hidden, salmap = m(mixed_img, aux, hidden, return_salmap=True, register_activations=True)\n",
    "            out[0][0][0].backward(retain_graph=m.use_rnn) # Steer\n",
    "            g = m.get_activations_gradient()\n",
    "            g = g[0].mean(0).cpu().numpy()\n",
    "            grads.append(g)\n",
    "\n",
    "        avg_grads = np.stack(grads).mean(0)\n",
    "        act_grad = avg_grads\n",
    "        \n",
    "        salmap = salmap[0].mean(0)\n",
    "        \n",
    "        act_grads.append(act_grad * salmap)\n",
    "        #act_grads.append(salmap) # This by itself is pretty nice\n",
    "        \n",
    "        imgs.append(img[0])\"\"\"\n",
    "\n",
    "    if use_training_wheels and i < TRAINING_WHEELS_WINDOW:\n",
    "        s = autopilot_controls\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def img_cam(act_grad, img, std):\n",
    "    # Gradients zero to one\n",
    "    act_grad = cv2.resize(act_grad, (img.shape[0],img.shape[1]))\n",
    "    mask = np.where(act_grad, (abs(act_grad)>std*2), 0)\n",
    "    mask = np.expand_dims(mask, -1)\n",
    "    \n",
    "    act_grad -= act_grad.min()\n",
    "    act_grad = act_grad / act_grad.max()\n",
    "    \n",
    "    # Make a three-channel heatmap out of the one channel gradients\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * act_grad), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    heatmap = heatmap * mask\n",
    "    heatmap = heatmap * 255\n",
    "    \n",
    "    #img = img / 255\n",
    "    \n",
    "    cam = heatmap*.5 + img\n",
    "    \n",
    "    #cam = cam / cam.max()\n",
    "    #cam = cam * 255\n",
    "    cam = np.clip(cam, 0, 255)\n",
    "    cam = cam.astype(np.uint8)\n",
    "    \n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beans/anaconda3/envs/procgen/lib/python3.7/site-packages/numpy/core/_methods.py:234: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/beans/anaconda3/envs/procgen/lib/python3.7/site-packages/numpy/core/_methods.py:195: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/beans/anaconda3/envs/procgen/lib/python3.7/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "std = np.array(act_grads[20:]).std()\n",
    "img_cams = []\n",
    "for img, act_grad in zip(imgs,act_grads):\n",
    "    img_cams.append(img_cam(act_grad,img,std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img1 = img_cams[0]\n",
    "height , width , layers = img1.shape\n",
    "fps = 10\n",
    "\n",
    "video = cv2.VideoWriter('cams.avi', cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (width,height))\n",
    "\n",
    "for i in range(len(img_cams)-1):\n",
    "    img = img_cams[i] # cv2 expects out of 255, integers\n",
    "    video.write(img)\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16, 29, 29)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
