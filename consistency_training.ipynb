{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Qxr8dm5Bnzk5"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/rgilman33/carlita.git\n",
    "# %cd carlita\n",
    "# !pip install -e .\n",
    "\n",
    "# !pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEqMfW9rjPc4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jMS2QTiynrv7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import threading\n",
    "\n",
    "from train_utils import *\n",
    "\n",
    "import gym3\n",
    "from procgen import ProcgenGym3Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A0dA8sTrnrv8"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "aD4KSAFBnrv8",
    "outputId": "b1e1c482-4a9e-4f76-820d-7bbc3f9d8528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pvjlCdjinrv9"
   },
   "outputs": [],
   "source": [
    "train_num_levels = 100_000 #500 #1500\n",
    "train_start_level = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EN_xA1XRnrv9",
    "outputId": "cb20c63b-7270-45df-b73d-ee69958bc0fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building procgen...done\n"
     ]
    }
   ],
   "source": [
    "bs = 128 \n",
    "\n",
    "env = ProcgenGym3Env(num=bs, env_name=\"testgame\", num_levels=train_num_levels, start_level=train_start_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMOyZUgRnrv9",
    "outputId": "af0c11de-ca02-4fb4-83fa-813744762de0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.85 s, sys: 120 ms, total: 3.97 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s = np.array([[.0,.0] for _ in range(bs)], dtype=np.float32)\n",
    "\n",
    "seq_len = 200\n",
    "\n",
    "for i in range(seq_len):\n",
    "    env.act(s)\n",
    "    rew, obs, first = env.observe()\n",
    "    img = obs['rgb']\n",
    "    info = env.get_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "iaTyl6Apnrv-",
    "outputId": "747b6137-6001-4974-e4b5-dfce9a4f3375"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc2ec184400>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EK61dD17nrv-",
    "outputId": "077f96ee-41bb-4610-a0b2-99a13b1ae6e2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queueing chunk of size torch.Size([100, 128, 3, 64, 64]) took 0.45 seconds\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(env=env, bs=bs, seq_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9P1KSjbJnrv-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-1SwEq8Cnrv_"
   },
   "outputs": [],
   "source": [
    "front, aux, target = dataloader.get_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QgUI-9Qnrv_",
    "outputId": "46e66307-88f6-4885-f5ad-7f3470fabce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queueing chunk of size torch.Size([100, 128, 3, 64, 64]) took 0.52 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 128, 3, 64, 64]),\n",
       " torch.Size([100, 128, 5]),\n",
       " torch.Size([100, 128, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front.shape, aux.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "fXnb9qmcoKZx",
    "outputId": "013f1a34-c000-4b11-b85b-2dca643109c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.hist(target[:,:,0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0laYY-7oY8b",
    "outputId": "a7468add-899c-464c-811a-713110f8e4fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5684), tensor(0.0080))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:,:,0].flatten().std(), target[:,:,0].flatten().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "q7m0pU8cJRxr"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from IPython.core.display import Image as JupyterImage\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "deg_to_rad = lambda x: x*0.0174533\n",
    "\n",
    "r = 20\n",
    "crop = transforms.CenterCrop(56)\n",
    "resize = transforms.Resize(64)\n",
    "color_jitter = transforms.ColorJitter(brightness=.5, contrast=.5, saturation=.5, hue=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxX94fQLYrsr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3fGbVPY-JZmz"
   },
   "outputs": [],
   "source": [
    "def get_rotated_view(front, aux, rotation):\n",
    "    front = front.clone()\n",
    "    SEQ_LEN, BS, C, H, W = front.shape\n",
    "    ff = front.reshape(SEQ_LEN*BS, C, H, W)\n",
    "    ff = transforms.RandomRotation([rotation,rotation])(ff)\n",
    "    ff = crop(ff)\n",
    "    ff = resize(ff)\n",
    "    # for i in range(len(ff)):\n",
    "    #    ff[i,:,:,:] = color_jitter(ff[i,:,:,:])\n",
    "    ff = ff.reshape(SEQ_LEN, BS, C, H, W)\n",
    "    _aux = aux.clone()\n",
    "    _aux[:,:,0] -= deg_to_rad(rotation)\n",
    "    return ff, _aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UuDB-mZ6NwBu"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "img should be PIL Image. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4b4e19097558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfront\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfront\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfront_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rotated_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfront\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-3e2f1276e5f2>\u001b[0m in \u001b[0;36mget_rotated_view\u001b[0;34m(front, aux, rotation)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfront\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfront\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomRotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/procgen/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0mangle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/procgen/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(img, angle, resample, expand, center)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "front = front[:32]\n",
    "aux = aux[:32]\n",
    "front_aug, aux_aug = get_rotated_view(front, aux, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bu_y7Jn5J7tv",
    "outputId": "d6178b2e-43be-4e72-ddaa-cb5d00176eba"
   },
   "outputs": [],
   "source": [
    "front_aug.shape, aux_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-69CaP3uauWV"
   },
   "outputs": [],
   "source": [
    "front_aug_2, aux_aug_2 = get_rotated_view(front, aux, -20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXvfrOwEa1t6",
    "outputId": "41a49bed-c4b1-4d08-e166-23a053955cf5"
   },
   "outputs": [],
   "source": [
    "front_aug_2.shape, aux_aug_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "j2UOjg6Xbaxg",
    "outputId": "99b2b702-edd4-4042-8d26-ad56aefbb17e"
   },
   "outputs": [],
   "source": [
    "save_image(front[0][:24].to(device), 'sample_image.png')\n",
    "JupyterImage('sample_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSMKpgXRbhKZ",
    "outputId": "d8b65d01-0d07-437d-8c16-2a4eec2563bf"
   },
   "outputs": [],
   "source": [
    "aux[0][:24,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "dCHrgomfJ_pd",
    "outputId": "8406d8e2-90ad-41ac-8d9d-2a595fc344f8"
   },
   "outputs": [],
   "source": [
    "save_image(front_aug[0][:24].to(device), 'sample_image.png')\n",
    "JupyterImage('sample_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86xKxvZdbssL",
    "outputId": "2d49ef54-e973-4fe8-b1a4-e7f6e5725092"
   },
   "outputs": [],
   "source": [
    "aux_aug[0][:24,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "ypRuw4CUONwJ",
    "outputId": "e16b875f-bed4-4cd4-df45-f296d16734ad"
   },
   "outputs": [],
   "source": [
    "save_image(front_aug_2[0][:24].to(device), 'sample_image.png')\n",
    "JupyterImage('sample_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzwjMglWbz9d",
    "outputId": "7db26cd0-830f-421a-f827-37a3dadae445"
   },
   "outputs": [],
   "source": [
    "aux_aug_2[0][:24,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah8yZZYQ4SPh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SkCYz1rYnrv_"
   },
   "outputs": [],
   "source": [
    "m = VizCNN(use_rnn=False).to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEpFfPzqnrv_",
    "outputId": "ede614af-ebac-4e8f-ebe5-e91a54e97ddf"
   },
   "outputs": [],
   "source": [
    "#m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "fGrfKBvHnrwA",
    "outputId": "e6696c46-1472-4b8a-9027-82cfb968d6ec"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-665f24598285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"m.torch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'm.torch'"
     ]
    }
   ],
   "source": [
    "m.load_state_dict(torch.load(\"m.torch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSKugff-nrwA",
    "outputId": "f403052b-f501-4ee7-8ff2-637a0e8372d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3192.834"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([torch.numel(p) for p in m.parameters()]) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "E8HQYuV-nrwA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden = get_hidden(bs)\n",
    "    out, hidden = m(front[:6].to(device), aux[:6].to(device), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUPX8Lec66aX"
   },
   "outputs": [],
   "source": [
    "out[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "rRS75Z5OnrwA"
   },
   "outputs": [],
   "source": [
    "#del front, aux, target, out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "ozmxrdm4nrwA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "pZn7t5ponrwA"
   },
   "outputs": [],
   "source": [
    "def testdrive(in_distribution=True, calibrate=False, use_training_wheels=False):\n",
    "    \n",
    "    TRAINING_WHEELS_WINDOW = 30\n",
    "    \n",
    "    t1 = time.time()\n",
    "    m.eval()\n",
    "    seq_len = 200\n",
    "    n_val = 100\n",
    "    val_env = ProcgenGym3Env(num=n_val, \n",
    "                            env_name=\"testgame\", \n",
    "                            num_levels=train_num_levels, \n",
    "                            start_level=train_start_level if in_distribution else train_start_level+train_num_levels)\n",
    "    \n",
    "    s = np.array([[.0,.0] for _ in range(n_val)], dtype=np.float32)\n",
    "    reward = 0\n",
    "    num_collisions = 0\n",
    "    wp_infractions = 0\n",
    "    successful_stops = 0\n",
    "    \n",
    "    hidden = get_hidden(n_val)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(seq_len):\n",
    "            val_env.act(s)\n",
    "            rew, obs, first = val_env.observe()\n",
    "            reward += rew.sum()\n",
    "            img = obs['rgb']\n",
    "            info = val_env.get_info()\n",
    "            num_collisions += np.array([e['collision'] for e in info]).sum()\n",
    "            wp_infractions += np.array([e['waypoint_infraction'] for e in info]).sum()\n",
    "            successful_stops += np.array([e['successful_stop'] for e in info]).sum()\n",
    "            \n",
    "            autopilot_control = np.array([[e[\"autopilot_\"+c] for c in control_properties] for e in info])\n",
    "            \n",
    "            aux = np.array([[e[a] for a in aux_properties] for e in info])\n",
    "\n",
    "            front = torch.from_numpy(img.astype(np.float32)/255.).unsqueeze(0).permute(0,1,4,2,3).to(device)\n",
    "            aux = torch.from_numpy(aux.astype(np.float32)).unsqueeze(0).to(device)\n",
    "            front, aux = get_rotated_view(front, aux, 0) # so crop matches train\n",
    "            \n",
    "            if calibrate:\n",
    "                s = autopilot_control\n",
    "            else:\n",
    "                out, hidden = m(front, aux, hidden)\n",
    "                s = out.squeeze(0).squeeze(-1).cpu().numpy()\n",
    "                s = np.clip(s, -5., 5.)\n",
    "                s[:,1]=.6 # not learning throttle for now\n",
    "                \n",
    "            if use_training_wheels and i < TRAINING_WHEELS_WINDOW:\n",
    "                s = autopilot_control\n",
    "        \n",
    "\n",
    "    reward /= (n_val*seq_len)\n",
    "    num_collisions /= (n_val*seq_len)\n",
    "    wp_infractions /= (n_val*seq_len)\n",
    "    successful_stops /= (n_val*seq_len)\n",
    "    \n",
    "    val_env.close()\n",
    "    m.train()\n",
    "    print(f\"validation took {round(time.time()-t1)} seconds\")\n",
    "    return reward, num_collisions, wp_infractions, successful_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_wGg5tZnrwB",
    "outputId": "bfa9aafd-aaac-40a0-99cc-92d506fcccda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation took 7 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1822, 0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdrive(in_distribution=True, calibrate=False, use_training_wheels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "LmV9vqtMnrwC"
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss().cuda()\n",
    "scaler = torch.cuda.amp.GradScaler() \n",
    "opt = torch.optim.Adam(m.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "Ky6etg7XnrwC",
    "outputId": "6661edcc-5d13-4706-f383-41e123b50e9e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dry-bee-128</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/rgilman33/carlita\" target=\"_blank\">https://wandb.ai/rgilman33/carlita</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/rgilman33/carlita/runs/2sl6h65z\" target=\"_blank\">https://wandb.ai/rgilman33/carlita/runs/2sl6h65z</a><br/>\n",
       "                Run data is saved locally in <code>/content/carlita/wandb/run-20210325_140756-2sl6h65z</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2sl6h65z)</h1><iframe src=\"https://wandb.ai/rgilman33/carlita/runs/2sl6h65z\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f64f3dbf510>"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"carlita\")\n",
    "# #wandb.watch(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SQaGWfZnrwC"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_epoch(train=True):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Caching baseline 'perfect' scores for val use. A bit awkward placement\n",
    "    autopilot_score_baseline_in_dist, autopilot_collisions_in_dist, _, ap_successful_stops_in = testdrive(in_distribution=True, calibrate=True)\n",
    "    #autopilot_score_baseline_out_dist, autopilot_collisions_out_dist, _, ap_successful_stops_out = testdrive(in_distribution=False, calibrate=True)\n",
    "    \n",
    "    m.train(train)\n",
    "    t1 = time.time()\n",
    "    epoch_loss, preds = [], []\n",
    "    #bs = random.choice(list(bs_bptt_lookup.keys()))\n",
    "    bptt = 1 #32 #random.choice([64, 72, 80, 88]) # increasing for 32x32 data #bs_bptt_lookup[bs]\n",
    "    \n",
    "    val_cadence = 4\n",
    "    log_cadence = 4\n",
    "    \n",
    "    global dataloader, bs\n",
    "    log_counter = 0\n",
    "    \n",
    "    while True:\n",
    "        chunk = dataloader.get_chunk() # Returns a chunk instantly, begins queuing up another chunk async\n",
    "        if not chunk: break\n",
    "        front_container, aux_container, target_container = chunk\n",
    "        chunk_len, bs, _, _, _ = front_container.shape\n",
    "        len_ix = 0\n",
    "        get_stats_container = lambda : {c:[] for c in [\"flip_consistency_loss\", \"rotation_consistency_loss\", \"steer_pred_std\",\"steer_pred_mean\", \"open_loop_loss\", \"output_shaping_loss\"]}\n",
    "        \n",
    "        chunk_stats = get_stats_container()\n",
    "\n",
    "        hidden = get_hidden(bs) # Resetting each chunk, ie each 800 steps or so. Only applicable w RNN\n",
    "        \n",
    "        clamp = lambda x: x #torch.clamp(x, -100, 100)\n",
    "\n",
    "        while len_ix < chunk_len:\n",
    "            \n",
    "            front = front_container[len_ix:len_ix+bptt, :, :, :, :].to(device).half()\n",
    "            aux = aux_container[len_ix:len_ix+bptt, :, :].to(device).half();\n",
    "            target = target_container[len_ix:len_ix+bptt, :, :].to(device).half()\n",
    "            len_ix += bptt\n",
    "\n",
    "            #######################\n",
    "            # # Supervised loss\n",
    "            # with torch.cuda.amp.autocast():\n",
    "            #     pred, hidden = m(front_aug_0, aux_aug_0, hidden)\n",
    "            #     pred = clamp(pred)\n",
    "            # sup_loss = loss_fn(target[:,:,1], pred[:,:,1]); \n",
    "\n",
    "            # max_steer = pred[:,:,1].max().item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred, hidden = m(front, aux, hidden)\n",
    "                    pred = clamp(pred)\n",
    "                open_loop_loss = ((target[:,:,0].detach() - pred[:,:,0])**2).mean()\n",
    "\n",
    "            steer_pred_mean, steer_pred_std = pred[:,:,0].flatten().mean(), pred[:,:,0].flatten().std()\n",
    "            #######################\n",
    "\n",
    "            #######################\n",
    "            # Unsupervised losses\n",
    "\n",
    "            # Base image\n",
    "            front_aug_0, aux_aug_0 = get_rotated_view(front, aux, 0)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                with torch.no_grad():\n",
    "                    pred_base, _ = m(front_aug_0, aux_aug_0, '')\n",
    "\n",
    "            with torch.cuda.amp.autocast():            \n",
    "                # Flip consistency\n",
    "                aux_horflip = aux_aug_0.clone()\n",
    "                aux_horflip[:,:,0]*=-1 # flip angle to wp\n",
    "                pred_horflip, _ = m(front_aug_0.flip(-1), aux_horflip, _)\n",
    "            flip_consistency_loss = loss_fn(pred_base[:,:,0], pred_horflip[:,:,0]*-1)\n",
    "\n",
    "\n",
    "            # # Output shaping\n",
    "            output_shaping_loss = 0\n",
    "\n",
    "            def get_output_shaping_loss(pred):\n",
    "                target_mean, target_std = 0, .77\n",
    "                steer_pred_mean, steer_pred_std = pred[:,:,0].flatten().mean(), pred[:,:,0].flatten().std()\n",
    "                mse = lambda x1, x2: (x1-x2)**2\n",
    "                l = mse(target_std, steer_pred_std) + mse(target_mean, steer_pred_mean)\n",
    "                return l\n",
    "            #output_shaping_loss += get_output_shaping_loss(pred_horflip)\n",
    "\n",
    "\n",
    "            # Rotation consistency \n",
    "            rotation_consistency_loss = 0\n",
    "            for i in range(5):\n",
    "                rotation = random.uniform(-20,20)\n",
    "                front_aug_1, aux_aug_1 = get_rotated_view(front, aux, rotation)\n",
    "                with torch.cuda.amp.autocast(): pred_rotate, _ = m(front_aug_1, aux_aug_1, '')\n",
    "                diff = pred_rotate[:,:,0] - pred_base[:,:,0]\n",
    "                diff_target = -torch.ones_like(diff) * deg_to_rad(rotation)\n",
    "                rotation_consistency_loss += loss_fn(diff, diff_target)\n",
    "                output_shaping_loss += get_output_shaping_loss(pred_rotate)\n",
    "            \n",
    "            rotation_consistency_loss /= 10\n",
    "            flip_consistency_loss /= 10_000\n",
    "            output_shaping_loss /= 1\n",
    "\n",
    "            loss = rotation_consistency_loss + output_shaping_loss + flip_consistency_loss \n",
    "\n",
    "            chunk_stats[\"flip_consistency_loss\"].append(flip_consistency_loss.item())\n",
    "            chunk_stats[\"rotation_consistency_loss\"].append(rotation_consistency_loss.item())\n",
    "            chunk_stats[\"output_shaping_loss\"].append(output_shaping_loss.item())\n",
    "            chunk_stats[\"steer_pred_std\"].append(steer_pred_std.item())\n",
    "            chunk_stats[\"steer_pred_mean\"].append(steer_pred_mean.item())\n",
    "            chunk_stats[\"open_loop_loss\"].append(open_loop_loss.item())\n",
    "\n",
    "            #######################\n",
    "                \n",
    "            if train:\n",
    "                # Scales the loss, and calls backward() to create scaled gradients \n",
    "                scaler.scale(loss).backward() \n",
    "                \n",
    "                # Unscales the gradients of optimizer's assigned params in-place\n",
    "                scaler.unscale_(opt)\n",
    "                # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "                torch.nn.utils.clip_grad_norm_(m.parameters(), 5.)\n",
    "        \n",
    "                # Unscales gradients and calls or skips optimizer.step() \n",
    "                scaler.step(opt) \n",
    "                # Updates the scale for next iteration \n",
    "                scaler.update() \n",
    "                opt.zero_grad()\n",
    "                \n",
    "            hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "\n",
    "        # Save and report at end of each chunk\n",
    "        t2 = time.time()\n",
    "        #chunk_loss = np.round(np.array(chunk_loss).mean(), 4)\n",
    "\n",
    "        for cc in chunk_stats.keys(): chunk_stats[cc] = np.round(np.array(chunk_stats[cc]).mean(), 8)\n",
    "        print(chunk_stats)\n",
    "        print(\"max m param\", max([p.max().item() for p in m.parameters()]))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        #epoch_loss.append(chunk_loss)\n",
    "        total_seconds = round(t2 - t1)\n",
    "        \n",
    "        if train and log_counter % log_cadence == 0 and log_counter>1: \n",
    "            \n",
    "            current_time = time.time()\n",
    "            torch.save(m.state_dict(), 'm.torch')\n",
    "            print(f'Done with chunk. Training took {total_seconds} seconds\\n')\n",
    "            \n",
    "        if log_counter % val_cadence == 0 and log_counter>1:\n",
    "            val_score_in_dist, collisions_in_dist, wp_infractions_in_dist, stops_in = testdrive(in_distribution=True, use_training_wheels=True) \n",
    "            val_score_in_dist /= autopilot_score_baseline_in_dist\n",
    "            \n",
    "            chunk_stats[\"val score\"] = np.round(val_score_in_dist,2)\n",
    "            print(\"val score\", chunk_stats[\"val score\"])\n",
    "\n",
    "            wandb.log(chunk_stats)\n",
    "        chunk_stats = get_stats_container()\n",
    "            \n",
    "        t1 = t2\n",
    "        log_counter+=1\n",
    "    \n",
    "    #loss = np.array(epoch_loss).mean()\n",
    "        \n",
    "    return '' #loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "I4XI2ggenrwG",
    "outputId": "c2d30b43-ed47-4d30-b7bc-9a7e360d4495",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-4c0aa5b2ebb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-0d568be8429f>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(train)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mrotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mfront_aug_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_aug_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rotated_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfront\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_rotate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfront_aug_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_aug_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_rotate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_base\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-747024a900d2>\u001b[0m in \u001b[0;36mget_rotated_view\u001b[0;34m(front, aux, rotation)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfront\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfront\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomRotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0mangle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(img, angle, interpolation, expand, center, fill, resample)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;31m# we need to set -angle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_inverse_affine_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional_tensor.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(img, matrix, interpolation, expand, fill)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0mexpand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m ) -> Tensor:\n\u001b[0;32m--> 687\u001b[0;31m     \u001b[0m_assert_grid_transform_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bilinear\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_output_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexpand\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional_tensor.py\u001b[0m in \u001b[0;36m_assert_grid_transform_inputs\u001b[0;34m(img, matrix, interpolation, fill, supported_interpolation_modes, coeffs)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;31m# Check fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0mnum_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_image_num_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         msg = (\"The number of elements in 'fill' cannot broadcast to match the number of \"\n\u001b[1;32m    555\u001b[0m                \"channels of the image ({} != {})\")\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQBw9znvnrwJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdNyhC0QnrwJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnMkMLRBgogz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ype2cu--gorA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KpcEzDqgown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsaVPR0kgo1B"
   },
   "outputs": [],
   "source": [
    "# Rotation consistency \n",
    "rotation = random.uniform(-20,20)\n",
    "front_aug_1, aux_aug_1 = get_rotated_view(front, aux, rotation)\n",
    "front_aug_2, aux_aug_2 = get_rotated_view(front, aux, -rotation)\n",
    "\n",
    "def get_rotation_consistency_loss(front_aug_1, aux_aug_1, front_aug_2, aux_aug_2):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            pred_rotate_1, _ = m(front_aug_1, aux_aug_1, '')\n",
    "            pred_rotate_1 = clamp(pred_rotate_1)\n",
    "            diff_1 = pred_rotate_1[:,:,0] - pred_base[:,:,0]\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        pred_rotate_2, _ = m(front_aug_2, aux_aug_2, '')\n",
    "        pred_rotate_2 = clamp(pred_rotate_2)\n",
    "        diff_2 = pred_rotate_2[:,:,0] - pred_base[:,:,0]\n",
    "        rotation_consistency_loss = loss_fn(diff_1, diff_2*-1)\n",
    "        #rotation_shift_loss = loss_fn((pred_rotate_1[:,:,0] + (-diff_1 * 2)), pred_rotate_2[:,:,0])\n",
    "        #rotation_consistency_loss += rotation_shift_loss\n",
    "\n",
    "    return rotation_consistency_loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "consistency training",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
